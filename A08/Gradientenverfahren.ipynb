{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bfbb4c5e",
   "metadata": {},
   "source": [
    "<figure>\n",
    "  <IMG SRC=\"https://upload.wikimedia.org/wikipedia/commons/thumb/d/d5/Fachhochschule_Südwestfalen_20xx_logo.svg/320px-Fachhochschule_Südwestfalen_20xx_logo.svg.png\" WIDTH=250 ALIGN=\"right\">\n",
    "</figure>\n",
    "\n",
    "# Einführung Machine Learning\n",
    "### Sommersemester 2022\n",
    "Prof. Dr. Heiner Giefers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Das Gradientenverfahren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Case\n",
    "m_c, n_c = 2, 4\n",
    "np.random.seed(0)\n",
    "X = np.random.randn(m_c, n_c)\n",
    "theta_c = np.r_[[[0]],np.random.randn(n_c, 1)]\n",
    "y_c = np.random.randn(m_c, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bei der Logistischen Regression haben wir das Gradientenverfahren benutzt, um die Parameter unseres Modells, einer linearen Funktion $$Z = f_{\\theta}(x)=\\theta_0+\\theta_ix$$ transformiert durch die Aktivierungsfunktion $$\\hat y = h_{\\theta}(x) = \\sigma(Z) = \\frac{1}{1+e^{-Z}}$$ schrittweise zu verbessern.\n",
    "\n",
    "\n",
    "Die Verbesserung, bzw. die Qualität des Modells, haben wir anhand der Kostenfunktion $$J_{\\theta}(x)=-\\frac{1}{m} \\sum\\limits_{i = 1}^{m} [y^{(i)}\\log(\\hat y^{(i)}) + (1-y^{(i)})\\log(1- \\hat y^{(i)})]$$ berechnet.\n",
    "\n",
    "Wir wollen nun schrittweise die Modellfunktion, die Kostenfunktion sowie das Gradientenverfahren als Python-Funktionen definieren.\n",
    "Um uns die Berechnungen zu vereinfachen, hängen wir an unseren Datensatz eine Spalte mit Einsen an.\n",
    "Damit können wir alle Parameter $\\theta$ (inklusive des Bias-Parameters) in einer Vektor-Operation verarbeiten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_c = np.c_[np.ones(X.shape[0]).T,X]\n",
    "X_c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modellfunktion\n",
    "**Aufgabe: Schreibe eine Funktion $f$, die folgende Parameter erhält:**\n",
    "1. Die Matrix $X \\in \\mathbb{R}^{m\\times{}n}$, die die Datenpunkte des Trainigsdatensatzes enthält.\n",
    "2. Die Parameter $\\theta$\n",
    "\n",
    "**$f$ soll folgende lineare Funktion implementieren:** $$Z = f_{\\theta}(X)=X\\theta$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a46e169efb324584",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def f(X, theta):\n",
    "    \"\"\"evaluates linear function.\n",
    "    Arguments:\n",
    "        X: value\n",
    "        theta: Parameter\n",
    "    \"\"\"\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-798b7b367b206c2f",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Test Cell\n",
    "#----------\n",
    "\n",
    "Z = f(X_c, theta_c)\n",
    "#----------\n",
    "# f\n",
    "\n",
    "assert Z.shape == (m_c, 1), 'Use correctly sequenced matrix multiplication'\n",
    "assert np.isclose(Z[0], 3.38207), 'Expected 3.38207 but got %.5f' %Z[0]\n",
    "\n",
    "del Z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Aufgabe: Implementieren Sie die Modellunktion $h$. Die Funktion $h$ soll die gleichen Parameter wie $f$ erhalten und die Funktion $f$ intern aufrufen. Das Ergebnis von $f$ soll durch die Sigmoid-Aktivierungsfunktion transformiert werden.:** $$\\hat y = h_{\\theta}(x) = \\sigma(Z) = \\frac{1}{1+e^{-Z}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-aed34bc92bba34fb",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def h(X, theta):\n",
    "    \"\"\"returns the sigmoid of the linear function.\n",
    "    Arguments:\n",
    "        X: Data\n",
    "        theta: Parameters\n",
    "    \"\"\"\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    return y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-3680cf0b32123e22",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Test Cell\n",
    "#----------\n",
    "\n",
    "y_hat = h(X_c, theta_c)\n",
    "#----------\n",
    "# h\n",
    "\n",
    "assert y_hat.shape == (m_c, 1)\n",
    "assert np.isclose(y_hat[0], 0.96713), 'Expected 0.96713 but got %.5f' %y_hat[0]\n",
    "\n",
    "del y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Aufgabe: Berechnen Sie nun die Kostenfunktion. Schreiben Sie eine Funktion $J$, die folgende Parameter erhält:**\n",
    "1. Die Matrix $X \\in \\mathbb{R}^{m\\times{}n}$, die die Datenpunkte des Trainigsdatensatzes enthält. .\n",
    "2. Die Parameter $\\theta$.\n",
    "3. Die Label $y$ in der Größe des Datensatzes `(m, 1)`.\n",
    "\n",
    "**$J$ berechnet die folgende Kostenfunktion:** $$J_{\\theta}(x)=-\\frac{1}{m} \\sum\\limits_{i = 1}^{m} [y^{(i)}\\log(\\hat y^{(i)}) + (1-y^{(i)})\\log(1- \\hat y^{(i)})]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-2441e8b76f6c8b3b",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def J(X,theta,y):\n",
    "    \"\"\"computes the Cross-entropy cost function\n",
    "    Arguments:\n",
    "        X: Data\n",
    "        theta: Parameter\n",
    "        y: True labels\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    return J.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-6d386b0ed985b5a1",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Test Cell\n",
    "#----------\n",
    "\n",
    "cost = J(X_c,theta_c,y_c)\n",
    "#----------\n",
    "# J\n",
    "\n",
    "assert cost.shape == (), 'Use correctly sequenced matrix multiplication'\n",
    "assert np.isclose(cost, 0.66739), 'Expected 0.66739but got %.5f' %cost\n",
    "\n",
    "del cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradientenverfahren\n",
    "\n",
    "**Schreibe für das Gradientenverfahren eine Funktion `grads`, die folgende Parameter erhält**\n",
    "1. Die Matrix $X \\in \\mathbb{R}^{m\\times{}n}$, die die Datenpunkte des Trainigsdatensatzes enthält. .\n",
    "2. Die Parameter $\\theta$.\n",
    "3. Die Label $y$ in der Größe des Datensatzes `(m, 1)`.\n",
    "\n",
    "**und den Gradienten $\\partial\\theta$ für die Parameter $\\theta$ berechnet** \n",
    "\n",
    "Dabei ist $\\partial\\theta$ ein Vektor der Dimension `(n+1, 1)` mit den Gradienten der Parameter: $$ \\partial \\theta = \\frac{1}{m}X^T(\\hat y-y)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-467565ebb154795c",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def grads(X,theta,y):\n",
    "    \"\"\"Berechnet die Gradienten der Kostenfunktion abhängig von dern Parametern.\n",
    "    Arguments:\n",
    "        X: Data\n",
    "        theta: Parameter\n",
    "        y: True labels\n",
    "    \"\"\"\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    return dtheta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-d5aaa7214b2dfbcd",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Test Cell\n",
    "#----------\n",
    "\n",
    "dt = grads(X_c,theta_c,y_c)\n",
    "#----------\n",
    "# grads\n",
    "\n",
    "assert dt.shape == theta_c.shape\n",
    "assert np.isclose(dt[1], 0.38273), 'Expected 0.38273 but got %.5f' %dt[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Aufgabe: Schreiben Sie nun eine Funktion, die die Modellparameter aufgrund der berechneten Gradienten aktualisiert.Die Funktion `update`erhält die Parameter $\\theta$, die Gradienten $\\partial \\theta$ sowie die Lernrate $\\alpha$ und berechnet:**\n",
    "\n",
    "$$\\theta = \\theta - \\alpha \\cdot \\partial \\theta$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-9c9c665b5ef00354",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def update(theta, dtheta, alpha):\n",
    "    \"\"\"updates parameters using gradient decent updating rule.\"\"\"\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-628e4db830095cf8",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Test Cell\n",
    "#----------\n",
    "\n",
    "t = update(theta_c, dt, 0.1)\n",
    "#----------\n",
    "# update\n",
    "\n",
    "assert t.shape == theta_c.shape\n",
    "assert np.isclose(t[1], -0.141491), 'Expected -0.141491 but got %.5f' %t[1]\n",
    "del t, dt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nun können wir das iterative Gradientenverfahren programmieren.\n",
    "**Aufgabe: Schreiben Sie eine Funktion `gradient_descent`, die folgende Parameter erhät:**\n",
    "1. Die Matrix $X \\in \\mathbb{R}^{m\\times{}n}$, die die Datenpunkte des Trainigsdatensatzes enthält. .\n",
    "2. Die Parameter $\\theta$.\n",
    "3. Die Label $y$ in der Größe des Datensatzes `(m, 1)`.\n",
    "4. Die Lernrate $\\alpha$.\n",
    "5. Die Anzahl der Iterationen.\n",
    "**Die Funktion soll die Trainierten Modellparameter $\\theta$ zurückgeben.**\n",
    "\n",
    "*Hinweis*: Berechnen Sie Kosten mit der Funktion `J` und hängen Sie diese Kosten nach jedem Berechnungsschritt and die Liste `cost` an $\\rightarrow$ Berechnen Sie die Gradienten mit der Funktion `grads` $\\rightarrow$ Verwenden Sie diese Gradienten um die Parameter mit der Funktion `update`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-c950a15eb73cda45",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def gradient_decent(X, theta, y, alpha=0.1, iterations=100):\n",
    "    \"\"\"performs gradient decent optimization.\n",
    "    Arguments:\n",
    "        X: Data\n",
    "        theta: Parameter\n",
    "        y: True labels\n",
    "        alpha(default=0.1): Learning rate\n",
    "        iterations(default=100): number of updating iterations\n",
    "    \"\"\"\n",
    "    \n",
    "    costs = []\n",
    "    \n",
    "    for i in range(iterations):\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "    return theta, costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-9e057a811aa97078",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Test Cell\n",
    "#----------\n",
    "\n",
    "t, costs = gradient_decent(X_c, theta_c, y_c)\n",
    "#----------\n",
    "# gradient_decent\n",
    "\n",
    "assert len(costs) == 100, 'Make sure to calculate and append the cost in every iteration.'\n",
    "assert np.isclose(t[4], 1.05769), 'Expected 1.05186 but got %.5f' %t[4]\n",
    "plt.plot([i for i in range(len(costs))],costs)\n",
    "del t, costs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anwendung der Funktionen auf einen realistischen Datensatz\n",
    "\n",
    "Wir habe nun alle Funktionen um unser logistisches Regressionsmodell für einen *echten* Datensatz zu einzusetzen.\n",
    "Wir verwenden hier den Brustkrebs-Datensatz aus Sklearn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "data = load_breast_cancer()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.data,data.target,test_size=0.3)\n",
    "\n",
    "# preprocessing\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "y_train = np.expand_dims(y_train, 1)\n",
    "y_test = np.expand_dims(y_test, 1)\n",
    "\n",
    "X_train = np.c_[np.ones(X_train.shape[0]).T,X_train]\n",
    "X_test = np.c_[np.ones(X_test.shape[0]).T,X_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing parameters\n",
    "theta = np.zeros((len(X_train[0]), 1))\n",
    "\n",
    "#training the model\n",
    "theta, costs = gradient_decent(X_train, theta, y_train.reshape(-1, 1), alpha = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(1,len(costs)),costs[1:], \"x-\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# measuring performance\n",
    "y_pred = (h(X_test,theta) >= 0.5)*1\n",
    "acc = 100-np.sum(np.abs(y_pred-y_test))*100/len(y_test)\n",
    "\n",
    "print(\"Die classifcation accuracy ist: \",acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
